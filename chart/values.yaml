# When set to true the chart will use values defined with production: sections over development: sections
# It was created for swapping between running the chart on a development kubernetes cluster with minimal 
# resources vs a production one.
is_production: false

# When scaling the _helpers.tpl function that is used for scaling out nodes will do a calculation based on this label
# Basically it will count the total number of nodes that have this label
# WARNING: If none of the nodes have this logstash replicas will get SET to 0
node_count_label:
  key: cnaps.io/node-type
  value: Tier-1

image:
  repository: ghcr.io/idaholab/malcolm
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion found in Chart.yml.
  tag: ""

auth:
  username: vagrant
  # Generated with the openssl passwd -1 <your password>
  openssl_password: "$1$9akV05vU$JHFoK3C2dDquTFAMoz14x/"
  # Generated with  htpasswd -bnB <your username> <your password>
  htpass_cred: "vagrant:$2y$05$kaHZ2j2VDDQCBf9bEk1gMuJAPRs4q4rolPRbq3zXUb7j2JyGuc50a"

opensearch:
  enabled: true
  url: http://opensearch:9200
  dashboards_url: http://dashboards:5601/dashboards
  development:
    java_memory: -Xms4g -Xmx4g -Xss256k -XX
  production:
    java_memory: -Xms32g -Xmx32g -Xss256k -XX
  
  
external_elasticsearch:
  enabled: false
  # Supports https or http urls for elasticsearch
  url: https://dataplane-ek-es-http.dataplane-ek.svc:9200
  username: "elastic"
  password: ""
  # Secret must be in following format if using elastic_secret_name:
  # apiVersion: v1
  #  data:
  #    elastic: passwordbase64encoded
  # kind: Secret
  # type: Opaque
  # metadata:
  #   name: somename
  #   namespace: somenamespace
  elastic_secret_name: "dataplane-ek-es-elastic-user"
  elastic_secret_namespace: "dataplane-ek"

  # Copy of the elastic certificate from elasticsearch namespace
  elastic_cert_name: "dataplane-ek-es-http-certs-public"
  elastic_cert_namespace: "dataplane-ek"

  # Only supports http urls for Kibana.
  dashboards_url: "http://dataplane-ek-kb-http.dataplane-ek.svc.cluster.local:5601"
  
logstash:
  # Note the number of logstash instances will be based on the number of nodes in your kubernetes cluster.
  development:
    java_memory: -Xms1g -Xmx1g -Xss1536k -XX  
    replicas: 1
    #See https://www.elastic.co/guide/en/logstash/current/tuning-logstash.html for details on these parameters.
    workers: 6
    batch_delay: 50
    batch_size: 75
  production:    
    java_memory: -Xms16g -Xmx16g -Xss1536k -XX
    workers: 18
    batch_delay: 50
    batch_size: 75

netbox:
  enabled: true
  # Set this this value to your reverse proxy url or leave it blank
  csrf_trusted_origins: https://malcolm.vp.bigbang.dev
  netbox_default_site: Cyberville

storage:
  # This helm chart requires a storage provisioner class it defaults to local-path provisioner
  # If your kuberenetes cluster has a different storage provisioner please ensure you change this name.
  # https://github.com/rancher/local-path-provisioner  
  pcap_claim: 
    size: 100Gi
    className: local-path
  zeek_claim: 
    size: 50Gi
    className: local-path
  suricata_claim:
    size: 50Gi
    className: local-path
  config_claim: 
    size: 25Gi
    className: local-path
  runtime_logs_claim: 
    size: 25Gi
    className: local-path
  opensearch_claim: 
    size: 100Gi
    className: local-path
  opensearch_backup_claim: 
    size: 100Gi
    className: local-path

ingress:
  # Enable this if deploying in RKE2 environment with the default nginx ingress controller
  enabled: false

istio:
  # Enable this if deploying in Kuberenetes environment leveraging istio service mesh
  # Additionally, it is recommended to turn off tls everywhere when this is set to true and let tls handle all things tls :).
  enabled: true
  gateway: istio-system/tenant
  virtualservicename: malcolm
  domain: vp.bigbang.dev

# Suricata live is enabled by default and will listen pcap_iface
# Defined under pcap_capture_env
# If you disable it, you will still be able to upload pcaps using the malcolm offline upload of pcaps functionality.
#https://docs.suricata.io/en/suricata-6.0.0/performance/tuning-considerations.html#af-packet AF_PACKET_RING_SIZE
suricata_live:
  enabled: true
  suricata_log_path: /opt/suricata/logs
  # Suricata live is a Daemonset but it will only schedule on kubernetes nodes that has following label
  nodeSelector:
    cnaps.io/suricata-capture: "true"

  development:
    af_packet_iface_threads: auto
    af_packet_ring_size: 2048
    # Used if is production is set to true
  production: 
    af_packet_iface_threads: 16
    # Default is calculated threads * max-pending-packets (Default of max-pending-packets is 1024)
    # This calculation was made with the assumtion that max-pending-packets is set to 1024
    af_packet_ring_size: 65536


suricata_offline:
  suricata_auto_analyze_pcap_threads: "1"

# Zeek live is enabled by default and will listen pcap_iface
# Defined under pcap_capture_env
# If you disable it, you will still be able to upload pcaps using the malcolm offline upload of pcaps functionality.
zeek_live:
  enabled: true
  zeek_log_path: /opt/zeek/logs  
  # Zeek live is a Daemonset but it will only schedule on kubernetes nodes that has following label
  nodeSelector:
    cnaps.io/zeek-capture: "true"

  development:
    zeek_lb_procs: "1"
    worker_lb_procs: "1"
    zeek_lb_method: "custom"
    zeek_af_packet_buffer_size: "67108864"
  production:
    zeek_lb_procs: "12"
    worker_lb_procs: "12"
    zeek_lb_method: "custom"
    zeek_af_packet_buffer_size: "134217728"
    

# Only affects the zeek offline deployment
zeek_offline:
  # The number of Zeek processes for analyzing uploaded PCAP files allowed
  #   to run concurrently
  zeek_auto_analyze_pcap_threads: 1

upload_common_env:
  # The age (in minutes) at which already-processed log files containing network traffic metadata should
  #   be pruned from the filesystem
  log_cleanup_minutes: 360
  # The age (in minutes) at which the compressed archives containing already-processed log files should
  #   be pruned from the filesystem
  zip_cleanup_minutes: 720

# Affects all zeek deployment live and offline
zeek_env:
  #Set to none, all or interesting.
  extract_mode: all
  extracted_file_enable_capa: true
  extracted_file_enable_clamav: true
  extracted_file_enable_yara: true
  extracted_file_http_server_enable: true
  extracted_file_http_server_encrypt: true
  extracted_file_preservation: quarantined
  extracted_file_max_bytes: "134217728"
  extracted_file_min_bytes: "64"


pcap_live:
  enabled: false  

  # Pcap live is a Daemonset but it will only schedule on kubernetes nodes that has following label
  nodeSelector:
    cnaps.io/arkime-capture: "true"

# Sets environment variables across multiple applications within Malcolm (IE: pcap capture, zeek and suricata)
# pcap_iface can be comma separate values.
pcap_capture_env:
  pcap_iface: ens192
  pcap_filter: ""
  # NOTE Net Sniff and tcpdump cannot be both true. Only one or the other can be running at a time.
  net_sniff: false
  tcpdump: true
  pcap_rotate_megabytes: 4096
  pcap_rotate_minutes: 10
  home_net: "[192.168.0.0/16,10.0.0.0/8,172.16.0.0/12]"


# Shared settings across all live capture type things (IE: Zeek live, Suricata live, and Pcap capture)
live_capture:
  tolerations:
  - key: node-role.kubernetes.io/control-plane
    operator: Exists
    effect: NoSchedule
  - key: node-role.kubernetes.io/master
    operator: Exists
    effect: NoSchedule
  - key: "cnaps.io/node-taint"
    operator: "Equal"
    value: "noncore"
    effect: "NoSchedule"

# Filebeat capture
filebeat:
  filebeat_clean_inactive: 180m  
  filebeat_close_inactive: 120s
  filebeat_close_inactive_live: 90m
  filebeat_ignore_older: 120m
  filebeat_scan_frequency: 10s

# Default affinity ensures that the filebeat Daemonset is applied with anything that also has suricata or zeek live capture running on it.
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: cnaps.io/suricata-capture
          operator: In
          values:
          - "true"
        - key: cnaps.io/zeek-capture
          operator: In
          values:
          - "true"

# Sets GID and UID for each container
process_env:
  pgid: 1000
  puid: 1000
