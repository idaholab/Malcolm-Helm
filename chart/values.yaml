# When set to true the chart will use values defined with production: sections over development: sections
# It was created for swapping between running the chart on a development kubernetes cluster with minimal
# resources vs a production one.
is_production: false

# When scaling the _helpers.tpl function that is used for scaling out nodes will do a calculation based on this label
# Basically it will count the total number of nodes that have this label
# WARNING: If none of the nodes have this logstash replicas will get SET to 0
node_count_label:
  key: cnaps.io/node-type
  value: Tier-1

image:
  repository: ghcr.io/idaholab/malcolm
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion found in Chart.yml.
  # Should only need to override these if something is awry with version supplied in Chart.yaml file.
  dirinit_tag: ""
  dashboards_tag: ""
  opensearch_tag: ""
  upload_tag: ""
  pcap_monitor_tag: ""
  arkime_tag: ""
  api_tag: ""
  zeek_tag: ""
  suricata_tag: ""
  dashboards_helper_tag: ""
  file_monitor_tag: ""
  filebeat_tag: ""
  logstash_tag: ""
  redis_tag: ""
  postgresql_tag: ""
  netbox_tag: ""
  htadmin_tag: ""
  pcap_capture_tag: ""
  freq_tag: ""
  nginx_tag: ""

auth:
  username: vagrant
  # Generated with the openssl passwd -1 <your password>
  openssl_password: "$1$9akV05vU$JHFoK3C2dDquTFAMoz14x/"
  # Generated with  htpasswd -bnB <your username> <your password>
  htpass_cred: "vagrant:$2y$05$kaHZ2j2VDDQCBf9bEk1gMuJAPRs4q4rolPRbq3zXUb7j2JyGuc50a"

opensearch:
  enabled: true
  singleNode: true
  replicas: 3
  url: http://opensearch:9200
  dashboards_url: http://dashboards:5601/dashboards
  development:
    java_memory: -Xms4g -Xmx4g -Xss256k -XX
  production:
    java_memory: -Xms32g -Xmx32g -Xss256k -XX
external_elasticsearch:
  enabled: false
  # Supports https or http urls for elasticsearch
  url: https://dataplane-ek-es-http.dataplane-ek.svc:9200
  username: "elastic"
  password: ""
  # Secret must be in following format if using elastic_secret_name:
  # apiVersion: v1
  #  data:
  #    elastic: passwordbase64encoded
  # kind: Secret
  # type: Opaque
  # metadata:
  #   name: somename
  #   namespace: somenamespace
  elastic_secret_name: "dataplane-ek-es-elastic-user"
  elastic_secret_namespace: "dataplane-ek"

  # Copy of the elastic certificate from elasticsearch namespace
  elastic_cert_name: "dataplane-ek-es-http-certs-public"
  elastic_cert_namespace: "dataplane-ek"

  # Only supports http urls for Kibana.
  dashboards_url: "http://dataplane-ek-kb-http.dataplane-ek.svc.cluster.local:5601"

logstash:
  # Note the number of logstash instances will be based on the number of nodes in your kubernetes cluster.
  development:
    java_memory: -Xms1g -Xmx1g -Xss1536k -XX
    replicas: 1
    #See https://www.elastic.co/guide/en/logstash/current/tuning-logstash.html for details on these parameters.
    workers: 6
    batch_delay: 50
    batch_size: 75
  production:
    java_memory: -Xms16g -Xmx16g -Xss1536k -XX
    workers: 18
    batch_delay: 50
    batch_size: 75

netbox:
  enabled: true
  # Set this this value to your reverse proxy url or leave it blank
  csrf_trusted_origins: https://malcolm.vp.bigbang.dev
  netbox_default_site: Cyberville
  netbox_auto_populate: false
  database:
   host: netbox-postgres
   name: netbox
   # Set true if using custom database for netbox
   is_custom: false
   # Create extra secrets for database, empty attributes will be set to randomly general postgres password.
   # The password will be same for all empty attributes.
   extra_secrets: [{}]
    # Example
    # - postgresql-secret:
    #     username: netbox
    #     password: ""
    # - pgpool-secret:
    #     adminUsername: netbox-pgpool
    #     adminPassword: ""

storage:
  # This helm chart requires a storage provisioner class it defaults to local-path provisioner
  # If your kuberenetes cluster has a different storage provisioner please ensure you change this name.
  # https://github.com/rancher/local-path-provisioner
  pcap_claim:
    size: 100Gi
    className: local-path
  zeek_claim:
    size: 50Gi
    className: local-path
  suricata_claim:
    size: 50Gi
    className: local-path
  config_claim:
    size: 25Gi
    className: local-path
  runtime_logs_claim:
    size: 25Gi
    className: local-path
  opensearch_claim:
    size: 25Gi
    className: local-path
  opensearch_backup_claim:
    size: 25Gi
    className: local-path
  netbox_postgres_claim:
    size: 15Gi
    className: local-path

ingress:
  # Enable this if deploying in RKE2 environment with the default nginx ingress controller
  enabled: false

istio:
  # Enable this if deploying in Kuberenetes environment leveraging istio service mesh
  # Additionally, it is recommended to turn off tls everywhere when this is set to true and let tls handle all things tls :).
  enabled: true
  gateway: istio-system/tenant
  virtualservicename: malcolm
  domain: vp.bigbang.dev

# Suricata live is enabled by default and will listen pcap_iface
# Defined under pcap_capture_env
# If you disable it, you will still be able to upload pcaps using the malcolm offline upload of pcaps functionality.
#https://docs.suricata.io/en/suricata-6.0.0/performance/tuning-considerations.html#af-packet AF_PACKET_RING_SIZE
suricata_live:
  enabled: true
  suricata_log_path: /opt/suricata/logs
  # Suricata live is a Daemonset but it will only schedule on kubernetes nodes that has following label
  nodeSelector:
    cnaps.io/suricata-capture: "true"

  development:
    af_packet_iface_threads: 2
    af_packet_ring_size: 2048
    max_pending_packets: 1024
    # Used if is production is set to true
  production:
    af_packet_iface_threads: 16
    # Default is calculated threads * max-pending-packets (Default of max-pending-packets is 1024)
    # This calculation was made with the assumtion that max-pending-packets is set to 1024
    af_packet_ring_size: 65536
    max_pending_packets: 4096

dashboards_helper_env:
  opensearch_index_size_prune_limit: 80%
  opensearch_index_size_prune_name_sort: false

arkime_env:
  free_space_g: "10%"

suricata_offline:
  suricata_auto_analyze_pcap_threads: "1"

# Zeek live is enabled by default and will listen pcap_iface
# Defined under pcap_capture_env
# If you disable it, you will still be able to upload pcaps using the malcolm offline upload of pcaps functionality.
zeek_live:
  enabled: true
  zeek_log_path: /opt/zeek/logs
  # Zeek live is a Daemonset but it will only schedule on kubernetes nodes that has following label
  nodeSelector:
    cnaps.io/zeek-capture: "true"

  development:
    zeek_lb_procs: "1"
    worker_lb_procs: "1"
    zeek_lb_method: "custom"
    zeek_af_packet_buffer_size: "67108864"
  production:
    zeek_lb_procs: "12"
    worker_lb_procs: "12"
    zeek_lb_method: "custom"
    zeek_af_packet_buffer_size: "134217728"


# Only affects the zeek offline deployment
zeek_offline:
  # The number of Zeek processes for analyzing uploaded PCAP files allowed
  #   to run concurrently
  zeek_auto_analyze_pcap_threads: 1

upload_common_env:
  # The age (in minutes) at which already-processed log files containing network traffic metadata should
  #   be pruned from the filesystem
  log_cleanup_minutes: 360
  # The age (in minutes) at which the compressed archives containing already-processed log files should
  #   be pruned from the filesystem
  zip_cleanup_minutes: 720

# Affects all zeek deployment live and offline
zeek_env:
  #Set to none, all or interesting.
  extract_mode: all
  extracted_file_enable_capa: true
  extracted_file_enable_clamav: true
  extracted_file_enable_yara: true
  extracted_file_http_server_enable: true
  extracted_file_http_server_encrypt: true
  extracted_file_preservation: quarantined
  development:
    capa_max_requests: 4
    clamd_max_requests: 8
    yara_max_requests: 8
    extracted_file_max_bytes: "134217728"
    extracted_file_min_bytes: "64"
  production:
    capa_max_requests: 8
    clamd_max_requests: 16
    yara_max_requests: 16
    extracted_file_max_bytes: "134217728"
    extracted_file_min_bytes: "64"

pcap_live:
  enabled: false

  # Pcap live is a Daemonset but it will only schedule on kubernetes nodes that has following label
  nodeSelector:
    cnaps.io/arkime-capture: "true"

# Sets environment variables across multiple applications within Malcolm (IE: pcap capture, zeek and suricata)
# pcap_iface can be comma separate values.
pcap_capture_env:
  pcap_iface: ens192
  pcap_filter: ""
  # NOTE Net Sniff and tcpdump cannot be both true. Only one or the other can be running at a time.
  net_sniff: false
  tcpdump: true
  pcap_rotate_megabytes: 4096
  pcap_rotate_minutes: 10
  home_net: "192.168.0.0/16,10.0.0.0/8,172.16.0.0/12"
  pcap_iface_tweak: true


# Shared settings across all live capture type things (IE: Zeek live, Suricata live, and Pcap capture)
live_capture:
  tolerations:
  - key: node-role.kubernetes.io/control-plane
    operator: Exists
    effect: NoSchedule
  - key: node-role.kubernetes.io/master
    operator: Exists
    effect: NoSchedule
  - key: "cnaps.io/node-taint"
    operator: "Equal"
    value: "noncore"
    effect: "NoSchedule"

# Filebeat capture
filebeat:
  filebeat_clean_inactive: 180m
  filebeat_close_inactive: 120s
  filebeat_close_inactive_live: 90m
  filebeat_ignore_older: 120m
  filebeat_scan_frequency: 10s

# Default affinity ensures that the filebeat Daemonset is applied with anything that also has suricata or zeek live capture running on it.
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: cnaps.io/suricata-capture
          operator: In
          values:
          - "true"
        - key: cnaps.io/zeek-capture
          operator: In
          values:
          - "true"

# Sets GID and UID for each container
process_env:
  pgid: 1000
  puid: 1000

# Only set to true if you intend on mounting these specific folders yourself using a kustomize overlay.
rule_mount_override:
  enabled: false