# When `true`, values under the `production:` sections will override those in `development:`.
# Use this to easily switch between lightweight development deployments and full production configurations.
is_production: false

# Enable Kubernetes NetworkPolicy resources to control pod ingress and egress traffic.
# When `true`, the chart creates default policies that restrict external access while allowing
# internal communication (and optional exceptions for Istio or external Elasticsearch).
enable_network_policies: false

# Determines how network traffic is captured and processed.
# Set to `live` for Arkime, Zeek, and Suricata to capture directly from the network interface,
# or `pipeline` to have Zeek and Suricata ingest PCAP data streamed from Arkime.
capture_mode: live

cluster:
  # Set to a CIDR mask that covers all the cluster nodes.
  node_cidr: "10.0.0.0/8"

# Label used by the _helpers.tpl scaling function to count nodes.
# The total number of nodes with this label determines the number of replicas.
# WARNING: If no nodes have this label, logstash replicas will be set to 0.
node_count_label:
  key: cnaps.io/node-type
  value: Tier-1

# Configuration for Custom CA Trust
# Specifies the name of a ConfigMap containing PEM-encoded CA certificates to be 
# trusted by the container OS.
#
# MECHANISM:
# 1. This ConfigMap is mounted to '/var/local/ca-trust/configmap' inside the container.
# 2. The entrypoint script ('docker-uid-gid-setup.sh') detects the 'PUSER_CA_TRUST' 
#    environment variable pointing to this location.
# 3. On startup, the script scans this directory, copies any certificates found 
#    into the OS trust store, and runs 'update-ca-trust' / 'update-ca-certificates'.
#
# REQUIREMENTS:
# - The ConfigMap keys (filenames) typically should end in .crt or .pem.
# - The values must be valid PEM-encoded certificate data.
# - If set to "var-local-catrust" (default), the chart creates an empty placeholder
#   configmap. If set to another name, will search for certificates within a ConfigMap
#   of that name and import/mount them accordingly.
ca_trust_configmap_name: "var-local-catrust"

# The StorageClass used for persistent volumes. Defaults to `local-path` (Local Path Provisioner).
# If your cluster doesn't support this, set `storage_class_name` to another class that supports
# ReadWriteMany. You can also override it at install time, e.g.:
#   helm install --set "storage_class_name=nfs-client"
storage_class_name: local-path

image:
  repository: ghcr.io/idaholab/malcolm
  pullPolicy: IfNotPresent

  # Overrides for the container image tags (default is the chart `appVersion` from Chart.yaml).
  # Only set this if you need a different version than the one specified in Chart.yaml.
  api_container_override: ""
  arkime_container_override: ""
  dashboards_container_override: ""
  dashboards_helper_container_override: ""
  dirinit_container_override: ""
  filebeat_container_override: ""
  filescan_container_override: ""
  freq_container_override: ""
  htadmin_container_override: ""
  keycloak_container_override: ""
  logstash_container_override: ""
  netbox_container_override: ""
  nginx_container_override: ""
  opensearch_container_override: ""
  pcap_capture_container_override: ""
  pcap_monitor_container_override: ""
  postgres_container_override: ""
  redis_container_override: ""
  strelka_backend_container_override: ""
  strelka_frontend_container_override: ""
  strelka_manager_container_override: ""
  suricata_container_override: ""
  upload_container_override: ""
  zeek_container_override: ""

auth:
  username: malcolm

  # Password for HTTP basic auth login, generated with `openssl passwd -1 <your password>`
  # default openssl_password is "malcolm" 
  openssl_password: "$1$xhZ5v90R$qQmoPuMUb1XAKitK0FoqO."

  # (The same) password for HTTP basic auth login, generated with `htpasswd -bnB <username> <password>`
  # default htpass_cred is malcolm:malcolm
  htpass_cred: "malcolm:$2y$05$G68ph4JvfyQu77luvKG77OvuKgeEgwPPtJk07L5iqwkifrLxRo1qi"

  # Optional: use an existing Kubernetes Secret instead of the default above, e.g.:
  #     kubectl create secret generic -n malcolm malcolm-auth \
  #       --from-literal=username="johndoe" \
  #       --from-literal=openssl_password="$(openssl passwd -1 'SuperSecretPassword' | tr -d '\n' | base64 | tr -d '\n')" \
  #       --from-literal=htpass_cred="$(htpasswd -bnB 'johndoe' 'SuperSecretPassword' | head -n1)"
  # then set existingSecret: "malcolm-auth"
  existingSecret: ""
  usernameKey: "username"
  opensslPassKey: "openssl_password"
  htpassCredKey: "htpass_cred"

  # Internal password hash secret for Arkime viewer cluster (see https://arkime.com/settings#passwordSecret)
  arkime_password: malcolm

  # If the Arkime WISE plugin is in use, this sets the configuration PIN for the WISE GUI
  # Users should change this value if using the WISE plugin GUI
  arkime_wise_pin: WISE2019

  # Authentication mode for accessing services. Options:
  #   - `basic` : htadmin auth
  #   - `keycloak` : embedded Keycloak
  #   - `keycloak_remote` : external Keycloak instance
  mode: "basic"

  # For `keycloak` or `keycloak_remote` modes: define required group/role and role-based access settings
  require_group: ""
  require_role: ""  
  role_based_access: "false"
  role_admin: "admin"
  role_read_access: "read_access"
  role_read_write_access: "read_write_access"  
  role_arkime_admin: "arkime_admin"
  role_arkime_read_access: "arkime_read_access"
  role_arkime_read_write_access: "arkime_read_write_access"
  role_arkime_pcap_access: "arkime_pcap_access"
  role_arkime_hunt_access: "arkime_hunt_access"  
  role_arkime_wise_read_access: "arkime_wise_read_access"
  role_arkime_wise_read_write_access: "arkime_wise_read_write_access"  
  role_dashboards_read_access: "dashboards_read_access"  
  role_dashboards_read_all_apps_access: "dashboards_read_all_apps_access"  
  role_dashboards_read_write_access: "dashboards_read_write_access"  
  role_dashboards_read_write_all_apps_access: "dashboards_read_write_all_apps_access"  
  role_extracted_files: "extracted_files"  
  role_netbox_read_access: "netbox_read_access"  
  role_netbox_read_write_access: "netbox_read_write_access"  
  role_upload: "upload"
  role_capture_service: "capture_service"

opensearch:
  enabled: true
  singleNode: true
  replicas: 3
  url: https://opensearch:9200
  dashboards_url: http://dashboards:5601/dashboards
  development:
    java_memory: -Xmx10g -Xms10g -Xss256k
    cluster_max_shards_per_node: 1000
  production:
    java_memory: -Xmx32g -Xms32g -Xss256k
    cluster_max_shards_per_node: 2500

external_elasticsearch:
  enabled: false

  # URL for the external Elasticsearch cluster. Can be an internal service or clusterIP,
  # and supports both `http` and `https`.
  url: https://dataplane-ek-es-http.dataplane-ek.svc:9200
  es_port: 9200
  kibana_port: 5601
  username: "malcolm"
  password: "malcolm_password"
  namespace: "dataplane-ek"

  # Kubernetes Secret format required when using `elastic_secret_name`:
  # apiVersion: v1
  #  data:
  #    password: passwordbase64encoded
  # kind: Secret
  # type: Opaque
  # metadata:
  #   name: somename
  #   namespace: somenamespace
  elastic_secret_name: "dataplane-ek-malcolm-es-auth"

  matchLabels:
    kibana:
      common.k8s.elastic.co/type: kibana
      kibana.k8s.elastic.co/name: dataplane-ek
    elastic:
      common.k8s.elastic.co/type: elasticsearch
      elasticsearch.k8s.elastic.co/cluster-name: dataplane-ek

  # Name and namespace of the Elasticsearch TLS certificate to use.
  # This should be a copy of the certificate from the Elasticsearch namespace.
  elastic_cert_name: "dataplane-ek-es-http-certs-public"
  elastic_cert_namespace: "dataplane-ek"

  # URL for accessing Kibana dashboards (only supports `http`).
  dashboards_url: "http://dataplane-ek-kb-http.dataplane-ek.svc.cluster.local:5601"

  # External URL for the Malcolm landing page dashboards.
  # Only used when `external_elasticsearch.enabled` is true.
  external_dashboards_url: https://dataplane-kibana.vp.bigbang.dev

# Shared environment variables for both Elasticsearch and OpenSearch
siem_env:

  # OpenSearch/Elasticsearch index patterns and timestamp fields
  logstash_override:

    # Enable this to modify the Logstash Elasticsearch/Opensearch output plugin
    # to point to an ILM rollover alias. The ILM policy and rollover_alias must
    # be defined outside of this deployment for correct operation.
    enabled: false
    ilm_policy: nta

    # Rollover aliases must match patterns in logstash_override.yml
    rollover_alias_zeek: malcolm_network_zeek
    rollover_alias_suricata: malcolm_network_suricata
    rollover_alias_beats: malcolm_beats

    # Default Beats pipeline and ILM policy
    default_beats_pipeline: sensor-status
    ilm_beats_policy: logs

    # Search aliases
    search_alias: malcolm_network
    other_search_alias: ""

    # Index settings
    number_of_shards: "2"
    number_of_replicas: "1"

  # Index pattern for network traffic logs (e.g., Zeek logs, Suricata alerts)
  malcolm_network_index_pattern: "arkime_sessions3-*"

  # Default time field for network traffic logs in Logstash and Dashboards
  malcolm_network_index_time_field: "firstPacket"

  # Suffix used for network traffic indices
  # Supports Ruby strftime strings in %{} (e.g., hourly: %{%y%m%dh%H}, daily: %{%y%m%d}, weekly: %{%yw%U}, monthly: %{%ym%m})
  # Supports expanding dot-delimited field names in {{ }} (e.g., {{event.provider}}%{%y%m%d})
  malcolm_network_index_suffix: "%{%y%m%d}"

  # Index pattern for other logs (e.g., nginx, Beats, Fluent Bit)
  malcolm_other_index_pattern: "malcolm_beats_*"

  # Default time field for other logs
  malcolm_other_index_time_field: "@timestamp"

  # Suffix for other logs (same rules as `malcolm_network_index_suffix`)
  malcolm_other_index_suffix: "%{%y%m%d}"

  # Index pattern specifically used by Arkime (usually matches `malcolm_network_index_pattern`)
  arkime_network_index_pattern: "arkime_sessions3-*"

  # Default time field for Arkime session indices
  arkime_network_index_time_field: "firstPacket"

  # If specified, overrides index.max_result_window which defines the maximum value of from + size for searches of the network data
  #   index, where from is the starting index to search from, and size is the number of results to return. CAUTION: increasing
  #   this value beyond its baked-in default (10000) may result may negatively impact performance.
  malcolm_index_max_result_window: ""

  # Optional: default pipeline for templates
  malcolm_template_default_pipeline: ""
  malcolm_beats_template_default_pipeline: ""

  # Optional: lifecycle parameters for templates
  malcolm_template_lifecycle_name: ""
  malcolm_template_lifecycle_rollover_alias: ""
  malcolm_beats_template_lifecycle_name: ""
  malcolm_beats_template_lifecycle_rollover_alias: ""

logstash:
  # Number of Logstash instances is determined by the number of nodes in your Kubernetes cluster.
  development:
    # Java memory settings for Logstash (heap size and stack size)
    java_memory: -Xmx3g -Xms3g -Xss2048k

    # Logstash pipeline tuning parameters
    # See: https://www.elastic.co/guide/en/logstash/current/tuning-logstash.html
    workers: 2
    batch_delay: 50
    batch_size: 75
    ordered: false

  production:
    java_memory: -Xmx16g -Xms16g -Xss2048k
    workers: 18
    batch_delay: 50
    batch_size: 75
    ordered: false

netbox:
  # Mode of NetBox integration. Options:
  #   - `local`    : use the embedded NetBox instance
  #   - `remote`   : connect to an external NetBox instance
  #   - `disabled` : do not use NetBox
  mode: "local"

  # Enable enrichment of data from NetBox
  enrichment: true
  netbox_default_site: "Malcolm"
  netbox_auto_populate: true
  netbox_cache_size: "10000"
  netbox_cache_ttl: "300"
  netbox_auto_populate_subnets: ""

  # URL of the remote NetBox instance (only used if `mode` is `remote`)
  url: ""

  # API token for the remote NetBox instance (only used if `mode` is `remote`)
  netbox_remote_token: ""

  # Reverse proxy URL for CSRF protection. Leave blank if not using a reverse proxy.
  csrf_trusted_origins: https://malcolm.vp.bigbang.dev

postgres:
  enabled: true

  # Hostname of the PostgreSQL service
  host: postgres

  # Default PostgreSQL password
  password: "ChangeMe"

  # Database names for applications
  netbox_db_name: "netbox"
  keycloak_db_name: "keycloak"

  # Set to `true` if using a custom database for NetBox
  is_custom: false

  # Create extra secrets for a custom database.
  # The same password value will be used for empty attributes.
  extra_secrets: [{}]

  # Example of extra secrets:
  # - postgresql-secret:
  #     username: netbox
  #     password: ""
  # - pgpool-secret:
  #     adminUsername: netbox-pgpool
  #     adminPassword: ""

storage:
  # Default storage class is defined by `storage_class_name` above.
  # Individual persistent volume claims can override the class using `classNameOverride`.
  development:
    pcap_claim:
      # Size of the persistent volume claim
      size: 25Gi
      # Optional override for the storage provisioner class
      classNameOverride: ""
    zeek_claim:
      size: 25Gi
      classNameOverride: ""
    suricata_claim:
      size: 25Gi
      classNameOverride: ""
    config_claim:
      size: 25Gi
      classNameOverride: ""
    runtime_logs_claim:
      size: 25Gi
      classNameOverride: ""
    opensearch_claim:
      size: 25Gi
      classNameOverride: ""
    opensearch_backup_claim:
      size: 25Gi
      classNameOverride: ""
    postgres_claim:
      size: 15Gi
      classNameOverride: ""
  production:
    pcap_claim:
      size: 100Gi
      classNameOverride: ""
    zeek_claim:
      size: 50Gi
      classNameOverride: ""
    suricata_claim:
      size: 50Gi
      classNameOverride: ""
    config_claim:
      size: 25Gi
      classNameOverride: ""
    runtime_logs_claim:
      size: 25Gi
      classNameOverride: ""
    opensearch_claim:
      size: 25Gi
      classNameOverride: ""
    opensearch_backup_claim:
      size: 25Gi
      classNameOverride: ""
    postgres_claim:
      size: 15Gi
      classNameOverride: ""

ingress:
  # Enable the default NGINX ingress controller
  enabled: true
  # Enable annotations bypassing proxy buffering
  proxyBufferBypass: true
  # Optional: specify additional annotations for the ingress
  # additionalAnnotations:
  #   cert-manager.io/cluster-issuer: "local-ca-issuer"
  additionalAnnotations: {}
  specRules:
    rules:
      - http:
          paths:
            - path: /
              pathType: Prefix
              backend:
                service:
                  name: nginx-proxy
                  port:
                    number: 443
        # Optional: specify an ingress hostname for the path above
        # host: malcolm.malcolmhost.network.local
  # Optional: specify ingress TLS settings
  # specTls:
  #   tls:
  #     - hosts:
  #         - malcolm.malcolmhost.network.local
  #       secretName: malcolm-tls

istio:
  # Enable if deploying in a Kubernetes environment with Istio service mesh.
  # When enabled, it is recommended to disable TLS everywhere and let Istio handle all TLS management.
  enabled: false
  gateway: istio-system/tenant
  virtualservicename: malcolm
  domain: vp.bigbang.dev
  istio_namespace: istio-system

# Configuration for Suricata, affecting both live and offline deployments
suricata_env:
  # Lua path for Suricata rules
  lua_path: "/opt/suricata/rules/?.lua;;"
  development:
    # Memory caps for various Suricata components in development
    stream_memcap: "1gb"
    host_memcap: "32mb"
    defrag_memcap: "32mb"
    ftp_memcap: "64mb"
    stream_reassembly_memcap: "256mb"
    flow_memcap: "128mb"
  production:
    # Memory caps for various Suricata components in production
    stream_memcap: "16gb"
    host_memcap: "32mb"
    defrag_memcap: "16gb"
    ftp_memcap: "4gb"
    stream_reassembly_memcap: "16gb"
    flow_memcap: "16gb"

# Suricata live is enabled by default and will listen on `pcap_iface` defined under `pcap_capture_env`
# If disabled, PCAPs can still be uploaded using the Malcolm offline upload functionality.
suricata_live:
  suricata_stats_enabled: true
  suricata_log_path: /opt/suricata/logs

  # Suricata live runs as a DaemonSet but schedules only on Kubernetes nodes with the following label
  nodeSelector:
    cnaps.io/suricata-capture: "true"

  # Tuning parameters, see:
  # - https://idaholab.github.io/Malcolm/docs/live-analysis.html#LiveAnalysisTuningSuricata
  # - https://docs.suricata.io/en/suricata-6.0.0/performance/tuning-considerations.html#af-packet

  development:
    af_packet_iface_threads: 2
    af_packet_ring_size: 2048
    af_packet_block_size: 32768
    af_packet_block_timeout: 100
    max_pending_packets: 1024
    eve_threaded: "true"
    eve_rotate_interval: "1h"

  production:
    af_packet_iface_threads: 32
    # Default is calculated threads * max_pending_packets (assumes max_pending_packets=1024)
    af_packet_ring_size: 131072
    af_packet_block_size: 1048576
    af_packet_block_timeout: 1000
    max_pending_packets: 30000
    eve_threaded: "true"
    eve_rotate_interval: "1h"

dashboards_helper_env:
  opensearch_index_size_prune_limit: 0
  opensearch_index_size_prune_name_sort: false
  dashboards_prefix: "Malcolm"
  dashboards_dark_mode: true
  dashboards_timepicker_from: "now-7d"
  dashboards_timepicker_to: "now"
  # The base URL of the Malcolm instance (e.g., https://malcolm.home.arpa), only used
  # where a remote Kibana instance needs absolute paths to request, for example, an extracted
  # file download or some other Malcolm-hosted asset.
  malcolm_url: https://malcolm.vp.bigbang.dev

suricata_offline:
  development:
    suricata_auto_analyze_pcap_threads: 0
    suricata_auto_analyze_pcap_processes: 1
  production:
    suricata_auto_analyze_pcap_threads: 0
    suricata_auto_analyze_pcap_processes: 10

# Zeek live is enabled by default and will listen on `pcap_iface` defined under `pcap_capture_env`
# If disabled, PCAPs can still be uploaded using the Malcolm offline upload functionality.
zeek_live:
  zeek_disable_stats: ""
  zeek_log_path: /usr/local/zeek/logs
  zeek_extracted_path: /usr/local/zeek/extracted
  filescan_log_path: /filescan
  filescan_redis_data_path: /redis

  # Zeek live runs as a DaemonSet but schedules only on Kubernetes nodes with the following label
  nodeSelector:
    cnaps.io/zeek-capture: "true"

  remoteNodeSelector:
    cnaps.io/zeek-remote-capture: "true"

  # Add additional ConfigMap- or Secret-based environment variables
  extra_envFrom: []
  # Add additional individual environment variables
  extra_env: []

  development:
    zeek_lb_procs_worker_default: ""
    zeek_lb_procs: "1"
    worker_lb_procs: "1"
    zeek_lb_method: "custom"
    zeek_af_packet_buffer_size: "67108864"
    zeek_pin_cpus_worker_1: ""

  production:
    zeek_lb_procs_worker_default: "8"
    zeek_lb_procs: "32"
    worker_lb_procs: "32"
    zeek_lb_method: "custom"
    zeek_af_packet_buffer_size: "134217728"
    zeek_pin_cpus_worker_1: ""


# Only affects the Zeek offline deployment
# NOTE: the zeek_intel_refresh_on_startup and zeek_intel_refresh_cron_expression 
#       is also applied to the zeek remote live sensors.
zeek_offline:
  # Refresh Zeek Intel data at startup
  zeek_intel_refresh_on_startup: "true"

  # Cron expression for refreshing Zeek Intel data
  zeek_intel_refresh_cron_expression: "0 */12 * * *"

  # Number of Zeek processes analyzing uploaded PCAPs concurrently
  development:
    zeek_auto_analyze_pcap_threads: 1
  production:
    zeek_auto_analyze_pcap_threads: 3

filescan_env:
  disable: false
  extracted_file_http_server_enable: true
  extracted_file_http_server_recursive: true
  extracted_file_http_server_magic: false
  extracted_file_preservation: quarantined
  development:
    # Prune ./zeek-logs/extract_files/ when it exceeds this size
    extracted_file_prune_threshold_max_size: 1TB
    # ... or when the total disk usage exceeds this percentage
    extracted_file_prune_threshold_total_disk_usage_percent: 0
    # Interval (seconds) to check whether to prune ./zeek-logs/extract_files/
    extracted_file_prune_interval_seconds: 300
    backend_procs: 1
  production:
    extracted_file_prune_threshold_max_size: 1TB
    extracted_file_prune_threshold_total_disk_usage_percent: 0
    extracted_file_prune_interval_seconds: 300
    backend_procs: 4

# Affects all Zeek deployments (live and offline)
zeek_env:
  # Determines which files are extracted from network traffic by Zeek
  # `none`       : no file extraction
  # `interesting`: extract files with MIME types of common attack vectors
  # `notcommtxt` : extract all files except common plain text files
  # `mapped`     : extract files with recognized MIME types
  # `known`      : extract files for which any MIME type can be determined
  # `all`        : extract all files
  extract_mode: interesting

  # When querying a TAXII or MISP feed, only process threat indicators
  # created or modified since this value. Can be a fixed date/time (e.g., 01/01/2021)
  # or a relative interval (e.g., 30 days ago). Blank disables filtering.
  zeek_intel_feed_since: ""

  # Value for Zeek's Intel::item_expiration timeout (-1min to disable)
  # `-1min` means intel items will never expire
  zeek_intel_item_expiration: "-1min"

  development:
    extracted_file_max_bytes: "134217728"
    clamd_max_threads: "8"
    clamd_max_queue: "200"
    clamd_max_conn_queue: "400"

  production:
    extracted_file_max_bytes: "134217728"
    clamd_max_threads: "8"
    clamd_max_queue: "200"
    clamd_max_conn_queue: "400"


maxmind:
  license_key: ""
  alternate_url: ""

# Configuration for Arkime, affecting both live and offline deployments
arkime_env:
  arkime_debug_level: "0"

  development:
    free_space_g: "10%"
    rotate_index: daily
    spi_data_max_indices: "7"
    arkime_init_shards: "1"
    arkime_init_replicas: "1"
    arkime_init_refresh_sec: ""
    arkime_init_shards_per_node: ""

    # Settings for Arkime's ILM/ISM features (https://arkime.com/faq#ilm)
    # Enable or disable Arkime index management
    index_management_enabled: "true"
    # Time before moving indices to warm and performing force merge (e.g., "30d" for 30 days)
    index_management_optimization_period: "30d"
    # Time before deleting indices (e.g., "90d" for 90 days)
    index_management_retention_time: "90d"
    # Number of replicas for older session indices
    index_management_older_session_replicas: "0"
    # Number of weeks of history to retain
    index_management_history_retention_weeks: "13"
    # Number of segments to optimize sessions for
    index_management_segments: "1"
    # Enable or disable hot/warm index design (non-session data in warm index)
    index_management_hot_warm_enabled: "false"

  production:
    free_space_g: "5%"
    rotate_index: daily
    spi_data_max_indices: "-1"
    arkime_init_shards: "2"
    arkime_init_replicas: "1"
    arkime_init_refresh_sec: ""
    arkime_init_shards_per_node: ""
    index_management_enabled: "true"
    index_management_optimization_period: "30d"
    index_management_retention_time: "90d"
    index_management_older_session_replicas: "0"
    index_management_history_retention_weeks: "13"
    index_management_segments: "1"
    index_management_hot_warm_enabled: "false"

arkime_live:
  enabled: true
  pcap_hostpath: "/zpool/nta-pcap"
  development:
    arkime_compression_type: none
    arkime_compression_level: "0"
    arkime_db_bulk_size: "4000000"
    arkime_max_packets_in_queue: "300000"
    arkime_packet_threads: "2"
    arkime_pcap_write_size: "2560000"
    arkime_tpacketv3_num_threads: "2"
    arkime_tpacketv3_block_size: "8388608"
  production:
    # See: https://arkime.com/settings#high-performance-settings
    arkime_compression_type: none
    arkime_compression_level: "3"
    arkime_db_bulk_size: "15000000"
    arkime_max_packets_in_queue: "300000"
    arkime_packet_threads: "6"
    arkime_pcap_write_size: "2560000"
    arkime_tpacketv3_num_threads: "2"
    arkime_tpacketv3_block_size: "8388608"

  # Arkime live runs as a DaemonSet but schedules only on Kubernetes nodes with the following label
  nodeSelector:
    cnaps.io/arkime-capture: "true"

pcap_live:
  enabled: false

  # PCAP live runs as a DaemonSet but schedules only on Kubernetes nodes with the following label
  nodeSelector:
    cnaps.io/pcap-capture: "true"

# Sets environment variables across multiple live capture applications (e.g., PCAP capture, Zeek, Suricata)
pcap_capture_env:
  # `pcap_iface` can accept comma-separated values.
  pcap_iface: ens192
  pcap_filter: ""
  # NOTE: netsniff and tcpdump cannot both be true. Only one can run at a time.
  netsniff: false
  tcpdump: true
  pcap_rotate_megabytes: 4
  pcap_rotate_minutes: 10
  home_net: "192.168.0.0/16,10.0.0.0/8,172.16.0.0/12"
  pcap_iface_tweak: true
  pcap_pipeline_verbosity: "-v"

# Shared settings across multiple live capture applications (e.g., PCAP capture, Zeek, Suricata)
live_capture:
  tolerations: []

upload_common_env:
  extra_tags: ""
  pcap_upload_max_file_gb: 50

# Filebeat log monitoring
filebeat:
  filebeat_clean_inactive: 180m
  filebeat_close_inactive: 120s
  filebeat_close_inactive_live: 90m
  filebeat_ignore_older: 120m
  filebeat_scan_frequency: 10s
  # Age (in minutes) at which already-processed log files containing network traffic metadata
  # will be pruned from the filesystem
  log_cleanup_minutes: "60"
  # Age (in minutes) at which compressed archives of already-processed log files
  # will be pruned from the filesystem
  zip_cleanup_minutes: "90"

  # Default affinity ensures the Filebeat DaemonSet runs on nodes also running Suricata or Zeek live capture.
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: cnaps.io/suricata-capture
          operator: In
          values:
          - "true"
      - matchExpressions:
        - key: cnaps.io/zeek-capture
          operator: In
          values:
          - "true"

# Sets the GID and UID for unprivileged processes in all containers
process_env:
  pgid: 1000
  puid: 1000

# Set to `true` only if you plan to mount rule folders manually via a Kustomize overlay.
rule_mount_override:
  enabled: false

# Settings specific to 26-pcap-processor.yml manifest
pcap_processor_env:
  nodeSelector:
    cnaps.io/arkime-capture: "true"
  
zeek_chart_overrides:
  serviceAccountName: ""
  sideCars: []
  offline_upload_volumes: []
  offline_upload_volumeMounts: []
  live_volumes: []
  live_volumeMounts: []
  live_remote_volumes: []
  live_remote_volumeMounts: []
  extra_init_containers: []
  live_extra_volumes: []  
  
suricata_chart_overrides:
  serviceAccountName: ""
  sideCars: []
  volumes: []
  volumeMounts: []
  live_volumes: []
  live_volumeMounts: []

kafka:
  enabled: "false"
  # Bootstrap broker or a comma-separated list of broker values (IP:PORT in both cases)
  brokers: "kafka.local:9091"
  topic: "zeek"  

# Keycloak settings below apply only when `auth.mode` is `keycloak` or `keycloak_remote`
keycloak:
  keycloak_auth_realm: "master"
  keycloak_auth_redirect_uri: "/index.html"
  keycloak_auth_url: ""
  keycloak_client_id: ""
  keycloak_client_secret: ""
  kc_cache: "local"
  kc_health_enabled: "true"
  kc_hostname: ""
  kc_hostname_strict: "false"
  kc_http_enabled: "true"
  kc_http_relative_path: "/keycloak"
  kc_metrics_enabled: "false"
  kc_proxy_headers: "xforwarded"
  kc_bootstrap_admin_username: "vagrant"
  kc_bootstrap_admin_password: "vagrant"

nginx:
  log_level: ""
  # use "type: ClusterIP" if using Ingress-NGINX as illustrated in 99-ingress-nginx.yml.example
  # use "type: LoadBalancer" if using AWS Load Balancer as illustrated in 99-ingress-alb.yml.example
  type: "ClusterIP"
