# When set to true the chart will use values defined with production: sections over development: sections
# It was created for swapping between running the chart on a development kubernetes cluster with minimal
# resources vs a production one.
is_production: false
enable_network_policies: false

# When scaling the _helpers.tpl function that is used for scaling out nodes will do a calculation based on this label
# Basically it will count the total number of nodes that have this label
# WARNING: If none of the nodes have this logstash replicas will get SET to 0
node_count_label:
  key: cnaps.io/node-type
  value: Tier-1

image:
  repository: ghcr.io/idaholab/malcolm
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion found in Chart.yml.
  # Should only need to override these if something is awry with version supplied in Chart.yaml file.
  dirinit_container_override: ""
  dashboards_container_override: ""
  opensearch_container_override: ""
  upload_container_override: ""
  pcap_monitor_container_override: ""
  arkime_container_override: ""
  api_container_override: ""
  zeek_container_override: ""
  suricata_container_override: ""
  dashboards_helper_container_override: ""
  file_monitor_container_override: ""
  filebeat_container_override: ""
  logsstash_container_override: ""
  keycloak_container_override: ""
  redis_container_override: ""
  postgres_container_override: ""
  netbox_container_override: ""
  htadmin_container_override: ""
  pcap_capture_container_override: ""
  freq_container_override: ""
  nginx_container_override: ""

auth:
  username: vagrant
  # Generated with the openssl passwd -1 <your password>
  openssl_password: "$1$9akV05vU$JHFoK3C2dDquTFAMoz14x/"
  # Generated with  htpasswd -bnB <your username> <your password>
  htpass_cred: "vagrant:$2y$05$kaHZ2j2VDDQCBf9bEk1gMuJAPRs4q4rolPRbq3zXUb7j2JyGuc50a"
  arkime_password: vagrant
  # Possible values are basic for htadmin auth, keycloak for embedded keycloak and finally keycloak_remote for remote keycloak instance.
  mode: "basic"

opensearch:
  enabled: true
  singleNode: true
  replicas: 3
  url: http://opensearch:9200
  dashboards_url: http://dashboards:5601/dashboards
  development:
    java_memory: -Xmx10g -Xms10g -Xss256k
  production:
    java_memory: -Xmx32g -Xms32g -Xss256k

external_elasticsearch:
  enabled: false
  # Internal url or clusterIP service and supports https or http urls for elasticsearch
  url: https://dataplane-ek-es-http.dataplane-ek.svc:9200
  es_port: 9200
  kibana_port: 5601
  username: "malcolm"
  password: ""
  # Secret must be in following format if using elastic_secret_name:
  # apiVersion: v1
  #  data:
  #    password: passwordbase64encoded
  # kind: Secret
  # type: Opaque
  # metadata:
  #   name: somename
  #   namespace: somenamespace
  namespace: "dataplane-ek"
  elastic_secret_name: "dataplane-ek-malcolm-es-auth"
  matchLabels:
    kibana:
      common.k8s.elastic.co/type: kibana
      kibana.k8s.elastic.co/name: dataplane-ek
    elastic:
      common.k8s.elastic.co/type: elasticsearch
      elasticsearch.k8s.elastic.co/cluster-name: dataplane-ek

  # Copy of the elastic certificate from elasticsearch namespace
  elastic_cert_name: "dataplane-ek-es-http-certs-public"
  elastic_cert_namespace: "dataplane-ek"

  # Only supports http urls for Kibana.
  dashboards_url: "http://dataplane-ek-kb-http.dataplane-ek.svc.cluster.local:5601"
  # Used for the Malcolm landing page and is only used if using a external_elasticsearch
  external_dashboards_url: https://dataplane-kibana.vp.bigbang.dev

# Shared environment varibles for both Elasticsearch and Opensearch
siem_env:
  # OpenSearch index patterns and timestamp fields
  # Index pattern for network traffic logs written via Logstash (e.g., Zeek logs, Suricata alerts)
  logstash_override:
    # When enabled the elasticsearch / opensearch logstash output plugin is modified to point to an ilm rollover alias
    # ILM policy and rollover_alias will need to be defined outside of this deployment for this to work as expected.
    enabled: false
    ilm_policy: nta
    # Must match the patterns located inside of logstash_override.yml
    rollover_alias_zeek: malcolm_network_zeek
    rollover_alias_suricata: malcolm_network_suricata
    search_alias: malcolm_network
  # Allows user to add a default_pipeline to malcolm_beat_template
  malcolm_beats_template_override:
    enabled: false
    default_pipeline: ""
  malcolm_network_index_pattern: "arkime_sessions3-*"
  # Default time field to use for network traffic logs in Logstash and Dashboards
  malcolm_network_index_time_field: "firstPacket"
  # Suffix used to create index to which network traffic logs are written
  #   * supports Ruby strftime strings in %{}; e.g.,
  #     - hourly: %{%y%m%dh%H}, twice daily: %{%P%y%m%d}, daily: %{%y%m%d}, weekly: %{%yw%U}, monthly: %{%ym%m})
  #   * supports expanding dot-delimited field names in {{ }}; e.g.,
  #     - {{event.provider}}%{%y%m%d}
  malcolm_network_index_suffix: "%{%y%m%d}"
  # Index pattern for other logs written via Logstash (e.g., nginx, beats, fluent-bit, etc.)
  malcolm_other_index_pattern: "malcolm_beats_*"
  # Default time field to use for other logs in Logstash and Dashboards
  malcolm_other_index_time_field: "@timestamp"
  # Suffix used to create index to which other logs are written (same rules as MALCOLM_NETWORK_INDEX_TIME_FIELD)
  malcolm_other_index_suffix: "%{%y%m%d}"
  # Index pattern used specifically by Arkime (will probably match MALCOLM_NETWORK_INDEX_PATTERN, should probably be arkime_sessions3-*)
  arkime_network_index_pattern: "arkime_sessions3-*"
  # Default time field used by for sessions in Arkime viewer
  arkime_network_index_time_field: "firstPacket"


logstash:
  # Note the number of logstash instances will be based on the number of nodes in your kubernetes cluster.
  development:
    java_memory: -Xmx2500m -Xms2500m -Xss2048k
    #See https://www.elastic.co/guide/en/logstash/current/tuning-logstash.html for details on these parameters.
    workers: 2
    batch_delay: 50
    batch_size: 75
  production:
    java_memory: -Xmx16g -Xms16g -Xss2048k
    workers: 18
    batch_delay: 50
    batch_size: 75

netbox:
  enabled: true
  # Set this this value to your reverse proxy url or leave it blank
  csrf_trusted_origins: https://malcolm.vp.bigbang.dev
  netbox_default_site: Cyberville
  netbox_auto_populate: true
  netbox_cache_size: "10000"
  netbox_cache_ttl: "300"

postgres:
  enabled: true
  host: postgres
  password: "ChangeMe"
  netbox_db_name: "netbox"
  keycloak_db_name: "keycloak"
  # Set true if using custom database for netbox
  is_custom: false
  # Create extra secrets for custom database, empty attributes will be set to password key.
  # The password will be same for all empty attributes.
  extra_secrets: [{}]
  # Example
  # - postgresql-secret:
  #     username: netbox
  #     password: ""
  # - pgpool-secret:
  #     adminUsername: netbox-pgpool
  #     adminPassword: ""

storage:
  # This helm chart requires a storage provisioner class it defaults to local-path provisioner
  # If your kuberenetes cluster has a different storage provisioner please ensure you change this name.
  # https://github.com/rancher/local-path-provisioner
  development:
    pcap_claim:
      # The size of the claim
      size: 25Gi
      # The kubernetes storage class name
      className: local-path
    zeek_claim:
      size: 25Gi
      className: local-path
    suricata_claim:
      size: 25Gi
      className: local-path
    config_claim:
      size: 25Gi
      className: local-path
    runtime_logs_claim:
      size: 25Gi
      className: local-path
    opensearch_claim:
      size: 25Gi
      className: local-path
    opensearch_backup_claim:
      size: 25Gi
      className: local-path
    netbox_postgres_claim:
      size: 15Gi
      className: local-path
  production:
    pcap_claim:
      size: 100Gi
      className: local-path
    zeek_claim:
      size: 50Gi
      className: local-path
    suricata_claim:
      size: 50Gi
      className: local-path
    config_claim:
      size: 25Gi
      className: local-path
    runtime_logs_claim:
      size: 25Gi
      className: local-path
    opensearch_claim:
      size: 25Gi
      className: local-path
    opensearch_backup_claim:
      size: 25Gi
      className: local-path
    netbox_postgres_claim:
      size: 15Gi
      className: local-path

ingress:
  # Enable this to use the default nginx ingress controller
  enabled: true
  specRules:
    rules:
    - http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: nginx-proxy
              port:
                number: 443
      # uncomment this line if you want to specify an ingress hostname with the path above
      # host: "malcolm.malcolmhost.network.local"

istio:
  # Enable this if deploying in Kuberenetes environment leveraging istio service mesh
  # Additionally, it is recommended to turn off tls everywhere when this is set to true and let istio handle all things tls :).
  enabled: false
  gateway: istio-system/tenant
  virtualservicename: malcolm
  domain: vp.bigbang.dev
  istio_namespace: istio-system


# Affects both the live and offline deployment of suricata
suricata_env:
  lua_path: "/opt/suricata/rules/?.lua;;"
  development:
    stream_memcap: "1gb"
    host_memcap: "32mb"
    defrag_memcap: "32mb"
    ftp_memcap: "64mb"
    stream_reassembly_memcap: "256mb"
    flow_memcap: "128mb"
  production:
    stream_memcap: "16gb"
    host_memcap: "32mb"
    defrag_memcap: "16gb"
    ftp_memcap: "4gb"
    stream_reassembly_memcap: "16gb"
    flow_memcap: "16gb"

# Suricata live is enabled by default and will listen pcap_iface
# Defined under pcap_capture_env
# If you disable it, you will still be able to upload pcaps using the malcolm offline upload of pcaps functionality.
#https://docs.suricata.io/en/suricata-6.0.0/performance/tuning-considerations.html#af-packet AF_PACKET_RING_SIZE
suricata_live:
  enabled: true
  suricata_stats_enabled: true
  suricata_log_path: /opt/suricata/logs
  # Suricata live is a Daemonset but it will only schedule on kubernetes nodes that has following label
  nodeSelector:
    cnaps.io/suricata-capture: "true"

  development:
    af_packet_iface_threads: 2
    af_packet_ring_size: 2048
    af_packet_block_size: 32768
    af_packet_block_timeout: 100
    max_pending_packets: 1024
    eve_threaded: "true"
    eve_rotate_interval: "1h"
    # Used if is production is set to true
  production:
    af_packet_iface_threads: 32
    # Default is calculated threads * max-pending-packets (Default of max-pending-packets is 1024)
    # This calculation was made with the assumtion that max-pending-packets is set to 1024
    af_packet_ring_size: 131072
    af_packet_block_size: 1048576
    af_packet_block_timeout: 1000
    max_pending_packets: 30000
    eve_threaded: "true"
    eve_rotate_interval: "1h"

dashboards_helper_env:
  opensearch_index_size_prune_limit: 0
  opensearch_index_size_prune_name_sort: false
  dashboards_prefix: "Malcolm"

suricata_offline:
  development:
    suricata_auto_analyze_pcap_threads: 1
  production:
    suricata_auto_analyze_pcap_threads: 3

# Zeek live is enabled by default and will listen pcap_iface
# Defined under pcap_capture_env
# If you disable it, you will still be able to upload pcaps using the malcolm offline upload of pcaps functionality.
zeek_live:
  enabled: true
  zeek_disable_stats: ""
  zeek_log_path: /opt/zeek/logs
  # Zeek live is a Daemonset but it will only schedule on kubernetes nodes that has following label
  nodeSelector:
    cnaps.io/zeek-capture: "true"

  development:
    zeek_lb_procs: "1"
    worker_lb_procs: "1"
    zeek_lb_method: "custom"
    zeek_af_packet_buffer_size: "67108864"
    zeek_pin_cpus_worker_1: ""
  production:
    zeek_lb_procs: "32"
    worker_lb_procs: "32"
    zeek_lb_method: "custom"
    zeek_af_packet_buffer_size: "134217728"
    zeek_pin_cpus_worker_1: ""


# Only affects the zeek offline deployment
zeek_offline:
  # The number of Zeek processes for analyzing uploaded PCAP files allowed
  #   to run concurrently
  zeek_intel_refresh_cron_expression: "0 */12 * * *"
  zeek_intel_refresh_on_startup: "true"
  development:
    zeek_auto_analyze_pcap_threads: 1
  production:
    zeek_auto_analyze_pcap_threads: 3


# Affects all zeek deployment live and offline
zeek_env:
  #Set to none, all or interesting.
  # `none`: no file extraction
  # `interesting`: extraction of files with mime types of common attack vectors
  # `notcommtxt`: extraction of all files except common plain text files
  # `mapped`: extraction of files with recognized mime types
  # `known`: extraction of files for which any mime type can be determined
  # `all`: extract all files
  extract_mode: interesting
  extracted_file_enable_capa: true
  extracted_file_enable_clamav: true
  extracted_file_enable_yara: true
  extracted_file_http_server_enable: true
  extracted_file_http_server_recursive: true
  extracted_file_preservation: quarantined
  # When querying a TAXII or MISP feed, only process threat indicators that have
  #   been created or modified since the time represented by this value;
  #   it may be either a fixed date/time (01/01/2021) or relative interval (30 days ago)
  # valid value "72 hours ago" blank means its disabled
  zeek_intel_feed_since: ""
  # Specifies the value for Zeek's Intel::item_expiration timeout (-1min to disable)
  # -1min means intel items will never expire
  zeek_intel_item_expiration: "-1min"
  # Specifies a cron expression indicating the refresh interval for generating the
  #   Zeek Intelligence Framework files ('' disables automatic refresh)
  # Valid value woudl be "0 */12 * * *" which would pull every 12 hours  
  development:
    capa_max_requests: 4
    clamd_max_requests: 8
    yara_max_requests: 8
    extracted_file_max_bytes: "134217728"
    extracted_file_min_bytes: "64"
    # Prune ./zeek-logs/extract_files/ when it exceeds this size...
    extracted_file_prune_threshold_max_size: 1TB
    # ... or when the *total* disk usage exceeds this percentage
    extracted_file_prune_threshold_total_disk_usage_percent: 0
    # Interval in seconds for checking whether to prune ./zeek-logs/extract_files/
    extracted_file_prune_interval_seconds: 300
  production:
    capa_max_requests: 8
    clamd_max_requests: 16
    yara_max_requests: 16
    extracted_file_max_bytes: "134217728"
    extracted_file_min_bytes: "64"
    extracted_file_prune_threshold_max_size: 1TB
    extracted_file_prune_threshold_total_disk_usage_percent: 0
    extracted_file_prune_interval_seconds: 300

maxmind:
  license_key: ""
  alternate_url: ""

# Variables apply to both offline and live version of arkime
arkime_env:
  arkime_debug_level: "0"
  development:
    free_space_g: "10%"
    rotate_index: daily
    # These variables manage setting for Arkime's ILM/ISM features (https://arkime.com/faq#ilm)
    # Whether or not Arkime should perform index management
    index_management_enabled: "false"
    # Time in hours/days before moving to warm and force merge (number followed by h or d)
    index_management_optimization_period: "30d"
    # Time in hours/days before deleting index (number followed by h or d)
    index_management_retention_time: "90d"
    # Number of replicas for older sessions indices
    index_management_older_session_replicas: "0"
    # Number of weeks of history to retain
    index_management_history_retention_weeks: "13"
    # Number of segments to optimize sessions for
    index_management_segments: "1"
    # Whether or not Arkime should use a hot/warm design (storing non-session data in a warm index)
    index_management_hot_warm_enabled: "false"
    spi_data_max_indices: "7"
  production:
    free_space_g: "20%"
    rotate_index: hourly
    index_management_enabled: "false"
    index_management_optimization_period: "30d"
    index_management_retention_time: "90d"
    index_management_older_session_replicas: "0"
    index_management_history_retention_weeks: "13"
    index_management_segments: "1"
    index_management_hot_warm_enabled: "false"
    spi_data_max_indices: "-1"

arkime_live:
  enabled: true
  pcap_hostpath: "/zpool/nta-pcap"
  development:
    arkime_compression_type: none
    arkime_compression_level: "0"
    arkime_db_bulk_size: "4000000"
    arkime_max_packets_in_queue: "300000"
    arkime_packet_threads: "2"
    arkime_pcap_write_size: "2560000"
    arkime_tpacketv3_num_threads: "2"
    arkime_tpacketv3_block_size: "8388608"
  production:
    arkime_compression_type: none
    arkime_compression_level: "3"
    arkime_db_bulk_size: "15000000"
    arkime_max_packets_in_queue: "300000"
    arkime_packet_threads: "6"
    arkime_pcap_write_size: "2560000"
    arkime_tpacketv3_num_threads: "2"
    arkime_tpacketv3_block_size: "8388608"

  # Arkime live is a Daemonset but it will only schedule on kubernetes nodes that has following label
  nodeSelector:
    cnaps.io/arkime-capture: "true"

pcap_live:
  enabled: false

  # Pcap live is a Daemonset but it will only schedule on kubernetes nodes that has following label
  nodeSelector:
    cnaps.io/pcap-capture: "true"

# Sets environment variables across multiple applications within Malcolm (IE: pcap capture, zeek and suricata)
# pcap_iface can be comma separate values.
pcap_capture_env:
  pcap_iface: ens192
  pcap_filter: ""
  # NOTE Net Sniff and tcpdump cannot be both true. Only one or the other can be running at a time.
  net_sniff: false
  tcpdump: true
  pcap_rotate_megabytes: 4096
  pcap_rotate_minutes: 10
  home_net: "192.168.0.0/16,10.0.0.0/8,172.16.0.0/12"
  pcap_iface_tweak: true


# Shared settings across all live capture type things (IE: Zeek live, Suricata live, and Pcap capture)
live_capture:
  tolerations: []

upload_common_env:
  extra_tags: "mykit1"

# Filebeat capture
filebeat:
  filebeat_clean_inactive: 180m
  filebeat_close_inactive: 120s
  filebeat_close_inactive_live: 90m
  filebeat_ignore_older: 120m
  filebeat_scan_frequency: 10s
  # The age (in minutes) at which already-processed log files containing network traffic metadata should
  #   be pruned from the filesystem
  log_cleanup_minutes: "60"
  # The age (in minutes) at which the compressed archives containing already-processed log files should
  #   be pruned from the filesystem
  zip_cleanup_minutes: "90"


# Default affinity ensures that the filebeat Daemonset is applied with anything that also has suricata or zeek live capture running on it.
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: cnaps.io/suricata-capture
          operator: In
          values:
          - "true"
      - matchExpressions:
        - key: cnaps.io/zeek-capture
          operator: In
          values:
          - "true"

# Sets GID and UID for each container
process_env:
  pgid: 1000
  puid: 1000

# Only set to true if you intend on mounting these specific folders yourself using a kustomize overlay.
rule_mount_override:
  enabled: false

zeek_chart_overrides:
  serviceAccountName: ""
  sideCars: []
  volumes: []
  volumeMounts: []
  live_volumes: []
  live_volumeMounts: []
  extra_init_containers: []
  live_extra_volumes: []
  live_extra_volumeMounts: []

suricata_chart_overrides:
  serviceAccountName: ""
  sideCars: []
  volumes: []
  volumeMounts: []
  live_volumes: []
  live_volumeMounts: []

kafka:
  enabled: "false"
  # This is either the bootstrap IP:PORT or a comma separated list of broker IP:PORT (s)
  brokers: "kafka.local:9091"
  topic: "zeek"  

# Below keycloak settings only apply when auth.mode is set to keycloak or keycloak_remote
keycloak:
  keycloak_auth_realm: "master"
  keycloak_auth_redirect_uri: "/index.html"
  keycloak_auth_url: "http://malcolm.lan/keycloak"
  keycloak_client_id: ""
  keycloak_client_secret: ""
  kc_cache: "local"
  kc_health_enabled: "true"
  kc_hostname: ""
  kc_hostname_strict: "false"
  kc_http_enabled: "true"
  kc_http_relative_path: "/keycloak"
  kc_metrics_enabled: "false"
  kc_proxy_headers: "xforwarded"
  kc_bootstrap_admin_username: "vagrant"
  kc_bootstrap_admin_password: "vagrant"
